一、	开发语言和环境
1、开发技术：
2、开发环境：
二、	系统分析
1、需求分析：解决手写数字集的识别问题，以及猫狗大战中对生活场景下猫和狗图片的识别。
2、系统结构：1. 获取数据，并做一定的预处理
				2. 构建神经网络，初始化权重参数，准备开始训练
				3. 将数据分成若干批次，代入模型跑一次前向运算并计算损失函数
				4. 反向传播求偏导数，得到损失函数对权重的梯度
				5. 使用优化算法修改权重，得到下一次前向传播的权重
				6. 重复3~5步，每完成一个epoch输出一次预测结果
				7. 完成设定次数的epoch后训练完成，得到最终模型，在此基础上对网络加以改进。
三、	主要代码：
4、读取数据集
  运行代码：
"""
   input_data.py: 读取训练数据
"""
# import tensorflow as tf
import tensorflow._api.v2.compat.v1 as tf
tf.disable_v2_behavior()
import numpy as np
import os


def get_files(file_dir):
   """
       输入：
           file_dir：存放训练图片的文件地址
       返回:
           image_list：乱序后的图片路径列表
           label_list：乱序后的标签(相对应图片)列表
   """
   # 建立空列表
   cats = []           # 存放是猫的图片路径地址
   label_cats = []     # 对应猫图片的标签
   dogs = []           # 存放是猫的图片路径地址
   label_dogs = []     # 对应狗图片的标签

   # 从file_dir路径下读取数据，存入空列表中
   for file in os.listdir(file_dir):     # file就是要读取的图片带后缀的文件名
       name = file.split(sep='.')        # 图片格式是cat.1.jpg / dog.2.jpg, 处理后name为[cat, 1, jpg]
       if name[0] == 'cat':              # name[0]获取图片名
           cats.append(file_dir + file)  # 若是cat，则将该图片路径地址添加到cats数组里
           label_cats.append(0)          # 并且对应的label_cats添加0标签 （这里记作：0为猫，1为狗）
       else:
           dogs.append(file_dir + file)
           label_dogs.append(1)          # 注意：这里添加进的标签是字符串格式，后面会转成int类型

   # print('There are %d cats\nThere are %d dogs' % (len(cats), len(dogs)))

   image_list = np.hstack((cats, dogs))               # 在水平方向平铺合成一个行向量，即两个数组的拼接
   label_list = np.hstack((label_cats, label_dogs))   # 这里把猫狗图片及标签合并分别存在image_list和label_list
   temp = np.array([image_list, label_list])  # 生成一个2 X 25000的数组，即2行、25000列
   temp = temp.transpose()                    # 转置向量，大小变成25000 X 2
   np.random.shuffle(temp)                    # 乱序，打乱这25000行排列的顺序

   image_list = list(temp[:, 0])              # 所有行，列=0（选中所有猫狗图片路径地址），即重新存入乱序后的猫狗图片路径
   label_list = list(temp[:, 1])              # 所有行，列=1（选中所有猫狗图片对应的标签），即重新存入乱序后的对应标签
   label_list = [int(float(i)) for i in label_list]  # 把标签列表转化为int类型（用列表解析式迭代，相当于精简的for循环）

   return image_list, label_list


def get_batch(image, label, image_W, image_H, batch_size, capacity):
   """
       输入：
           image,label：要生成batch的图像和标签
           image_W，image_H: 图像的宽度和高度
           batch_size: 每个batch（小批次）有多少张图片数据
           capacity: 队列的最大容量
       返回：
           image_batch: 4D tensor [batch_size, width, height, 3], dtype=tf.float32
           label_batch: 1D tensor [batch_size], dtype=tf.int32
   """
   image = tf.cast(image, tf.string)   # 将列表转换成tf能够识别的格式
   label = tf.cast(label, tf.int32)

   # 队列的理解：
   #     每次训练时，从队列中取一个batch送到网络进行训练，然后又有新的图片从训练库中注入队列，这样循环往复。
   #     队列相当于起到了训练库到网络模型间数据管道的作用，训练数据通过队列送入网络。
   input_queue = tf.train.slice_input_producer([image, label])   # 生成队列(牵扯到线程概念，便于batch训练), 将image和label传入
   # input_queue = tf.optimizers.slice_input_producer([image, label])   # Tensorflow 2.0版本

   label = input_queue[1]
   image_contents = tf.read_file(input_queue[0])              # 图像的读取需要tf.read_file(), 标签则可以直接赋值。
   image = tf.image.decode_jpeg(image_contents, channels=3)   # 使用JPEG的格式解码从而得到图像对应的三维矩阵。
   # 注意：这里image解码出来的数据类型是uint8, 之后模型卷积层里面conv2d()要求传入数据为float32类型

   # 图片数据预处理：统一图片大小(缩小图片) + 标准化处理
   # ResizeMethod.NEAREST_NEIGHBOR：最近邻插值法，将变换后的图像中的原像素点最邻近像素的灰度值赋给原像素点的方法，返回图像张量dtype与所传入的相同。
   image = tf.image.resize_images(image, [image_H, image_W], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
   image = tf.cast(image, tf.float32)                  # 将image转换成float32类型
   image = tf.image.per_image_standardization(image)   # 图片标准化处理，加速神经网络的训练

   # 按顺序读取队列中的数据
   image_batch, label_batch = tf.train.batch([image, label],          # 进队列的tensor列表数据
                                             batch_size=batch_size,   # 设置每次从队列中获取出队数据的数量
                                             num_threads=64,          # 涉及到线程，配合队列
                                             capacity=capacity)       # 用来设置队列中元素的最大数量

   return image_batch, label_batch

5、卷积神经模型
运行代码：
"""
   model.py: CNN神经网络模型
"""
# import tensorflow as tf
import tensorflow._api.v2.compat.v1 as tf
tf.disable_v2_behavior()

def cnn_inference(images, batch_size, n_classes):
   """
       输入：
           images：队列中取的一批图片, 具体为：4D tensor [batch_size, width, height, 3]
           batch_size：每个批次的大小
           n_classes：n分类（这里是二分类，猫或狗）
       返回：
           softmax_linear：表示图片列表中的每张图片分别是猫或狗的预测概率（即：神经网络计算得到的输出值）。
                           例如: [[0.459, 0.541], ..., [0.892, 0.108]],
                           一个数值代表属于猫的概率，一个数值代表属于狗的概率，两者的和为1。
   """

   # TensorFlow中的变量作用域机制：
   #       tf.variable_scope(<scope_name>): 指定命名空间
   #       tf.get_variable(<name>, <shape>, <dtype>, <initializer>): 创建一个变量

   # 第一层的卷积层conv1，卷积核(weights)的大小是 3*3, 输入的channel(管道数/深度)为3, 共有16个
   with tf.variable_scope('conv1') as scope:
       # tf.truncated_normal_initializer():weights初始化生成截断正态分布的随机数，stddev标准差
       weights = tf.get_variable('weights',
                                 shape=[3, 3, 3, 16],
                                 dtype=tf.float32,
                                 initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))
       biases = tf.get_variable('biases',
                                shape=[16],
                                dtype=tf.float32,
                                initializer=tf.constant_initializer(0.1))   # 初始化为常数，通常偏置项biases就是用它初始化的

       # strides = [1, y_movement, x_movement, 1], 每个维度的滑动窗口的步幅,一般首末位置固定都为1
       # padding = 'SAME', 是考虑边界, 不足时用0去填充周围
       # padding = 'VALID', 不考虑边界, 不足时舍弃不填充周围
       # 参考：https://blog.csdn.net/qq_36201400/article/details/108454066
       # 输入的images是[16,208,208,3], 即16张 208*208 大小的图片, 图像通道数是3
       # weights(卷积核)的大小是 3*3, 数量为16
       # strides(滑动步长)是[1,1,1,], 即卷积核在图片上卷积时分别向x、y方向移动为1个单位
       # 由于padding='SAME'考虑边界，最后得到16张图且每张图得到16个 208*208 的feature map(特征图)
       # conv(最后输出的结果)是shape为[16,208,208,16]的4维张量(矩阵/向量)
       # 用weights卷积核对images图片进行卷积
       conv = tf.nn.conv2d(images, weights, strides=[1, 1, 1, 1], padding='SAME')
       pre_activation = tf.nn.bias_add(conv, biases)      # 加入偏差，biases向量与矩阵的每一行进行相加, shape不变
       conv1 = tf.nn.relu(pre_activation, name='conv1')   # 在conv1的命名空间里，用relu激活函数非线性化处理

   # 第一层的池化层pool1和规范化norm1(特征缩放）
   with tf.variable_scope('pooling1_lrn') as scope:
       # 对conv1池化得到feature map
       # 参考：https://blog.csdn.net/qq_22968719/article/details/88318626
       pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],
                              padding='SAME', name='pooling1')
       # lrn()：局部响应归一化, 一种防止过拟合的方法, 增强了模型的泛化能力，
       norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0,
                         beta=0.75, name='norm1')

   # 第二层的卷积层cov2，卷积核(weights)的大小是 3*3, 输入的channel(管道数/深度)为16, 共有16个
   with tf.variable_scope('conv2') as scope:
       weights = tf.get_variable('weights',
                                 shape=[3, 3, 16, 16],  # 这里的第三位数字16需要等于上一层的tensor维度
                                 dtype=tf.float32,
                                 initializer=tf.truncated_normal_initializer(stddev=0.1, dtype=tf.float32))
       biases = tf.get_variable('biases',
                                shape=[16],
                                dtype=tf.float32,
                                initializer=tf.constant_initializer(0.1))
       conv = tf.nn.conv2d(norm1, weights, strides=[1, 1, 1, 1], padding='SAME')
       pre_activation = tf.nn.bias_add(conv, biases)
       conv2 = tf.nn.relu(pre_activation, name='conv2')

   # 第二层的池化层pool2和规范化norm2(特征缩放）
   with tf.variable_scope('pooling2_lrn') as scope:
       # 这里选择了先规范化再池化
       norm2 = tf.nn.lrn(conv2, depth_radius=4, bias=1.0, alpha=0.001/9.0,
                         beta=0.75, name='norm2')
       pool2 = tf.nn.max_pool(norm2, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1],
                              padding='SAME', name='pooling2')

   # 第三层为全连接层local3
   # 连接所有的特征, 将输出值给分类器 (将特征映射到样本标记空间), 该层映射出256个输出
   with tf.variable_scope('local3') as scope:
       # 将pool2张量铺平, 再把维度调整成shape(shape里的-1, 程序运行时会自动计算填充)
       # 参考：https://blog.csdn.net/csdn0006/article/details/106238909/
       reshape = tf.reshape(pool2, shape=[batch_size, -1])

       dim = reshape.get_shape()[1].value            # 获取reshape后的列数
       weights = tf.get_variable('weights',
                                 shape=[dim, 256],   # 连接256个神经元
                                 dtype=tf.float32,
                                 initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))
       biases = tf.get_variable('biases',
                                shape=[256],
                                dtype=tf.float32,
                                initializer=tf.constant_initializer(0.1))
       # 矩阵相乘再加上biases，用relu激活函数非线性化处理
       local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name='local3')

   # 第四层为全连接层local4
   # 连接所有的特征, 将输出值给分类器 (将特征映射到样本标记空间), 该层映射出512个输出
   with tf.variable_scope('local4') as scope:
       weights = tf.get_variable('weights',
                                 shape=[256, 512],  # 再连接512个神经元
                                 dtype=tf.float32,
                                 initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))
       biases = tf.get_variable('biases',
                                shape=[512],
                                dtype=tf.float32,
                                initializer=tf.constant_initializer(0.1))
       # 矩阵相乘再加上biases，用relu激活函数非线性化处理
       local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name='local4')

   # 第五层为输出层(回归层): softmax_linear
   # 将前面的全连接层的输出，做一个线性回归，计算出每一类的得分，在这里是2类，所以这个层输出的是两个得分。
   with tf.variable_scope('softmax_linear') as scope:
       weights = tf.get_variable('weights',
                                 shape=[512, n_classes],
                                 dtype=tf.float32,
                                 initializer=tf.truncated_normal_initializer(stddev=0.005, dtype=tf.float32))
       biases = tf.get_variable('biases',
                                shape=[n_classes],
                                dtype=tf.float32,
                                initializer=tf.constant_initializer(0.1))

       # softmax_linear的行数=local4的行数，列数=weights的列数=bias的行数=需要分类的个数
       # 经过softmax函数用于分类过程中，它将多个神经元的输出，映射到（0,1）区间内，可以看成概率来理解
       # 这里local4与weights矩阵相乘，再矩阵相加biases
       softmax_linear = tf.add(tf.matmul(local4, weights), biases, name='softmax_linear')

   # 这里没做归一化和交叉熵。真正的softmax函数放在下面的losses()里面和交叉熵结合在一起了，这样可以提高运算速度。
   # 图片列表中的每张图片分别被每个分类取到的概率，
   return softmax_linear


def losses(logits, labels):
   """
       输入：
           logits: 经过cnn_inference得到的神经网络输出值（图片列表中每张图片分别是猫或狗的预测概率）
           labels: 图片对应的标签（即：真实值。用于与logits预测值进行对比得到loss）
       返回：
           loss： 损失值（label真实值与神经网络输出预测值之间的误差）
   """
   with tf.variable_scope('loss') as scope:
       # label与神经网络输出层的输出结果做对比，得到损失值（这做了归一化和交叉熵处理）
       cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='loss_per_eg')
       loss = tf.reduce_mean(cross_entropy, name='loss')  # 求得batch的平均loss（每批有16张图）
   return loss


def training(loss, learning_rate):
   """
       输入：
           loss: 训练中得到的损失值
           learning_rate：学习率
       返回：
           train_op: 训练的最优值。训练op，这个参数要输入sess.run中让模型去训练。
   """
   with tf.name_scope('optimizer'):
       # tf.train.AdamOptimizer():
       # 除了利用反向传播算法对权重和偏置项进行修正外，也在运行中不断修正学习率。
       # 根据其损失量学习自适应，损失量大则学习率越大，进行修正的幅度也越大;
       #                     损失量小则学习率越小，进行修正的幅度也越小，但是不会超过自己所设定的学习率。
       optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)    # 使用AdamOptimizer优化器来使loss朝着变小的方向优化

       global_step = tf.Variable(0, name='global_step', trainable=False)  # 全局步数赋值为0

       # loss：即最小化的目标变量，一般就是训练的目标函数，均方差或者交叉熵
       # global_step：梯度下降一次加1，一般用于记录迭代优化的次数，主要用于参数输出和保存
       train_op = optimizer.minimize(loss, global_step=global_step)   # 以最大限度地最小化loss

   return train_op


def evaluation(logits, labels):
   """
       输入：
           logits: 经过cnn_inference得到的神经网络输出值（图片列表中每张图片分别是猫或狗的预测概率）
           labels: 图片对应的标签（真实值，0或1）
       返回：
           accuracy：准确率（当前step的平均准确率。即：这些batch中多少张图片被正确分类了）
   """
   with tf.variable_scope('accuracy') as scope:
       correct = tf.nn.in_top_k(logits, labels, 1)   # 用法参考：https://www.cnblogs.com/logo-88/p/9099383.html
       correct = tf.cast(correct, tf.float16)        # 转换格式为浮点数
       accuracy = tf.reduce_mean(correct)            # 计算当前批的平均准确率
   return accuracy

6、模型的训练与评估
"""
   training.py: 模型的训练及评估
"""
# import tensorflow as tf
import tensorflow._api.v2.compat.v1 as tf
tf.disable_v2_behavior()
import os
import numpy as np
import matplotlib.pyplot as plt
import input_data
import model


N_CLASSES = 2  # 分类数，猫和狗
IMG_W = 208  # resize图片宽高，太大的话训练时间久
IMG_H = 208
BATCH_SIZE = 16  # 每批次读取数据的数量
CAPACITY = 2000  # 队列最大容量
MAX_STEP = 10000  # 训练最大步数，一般5K~10k
learning_rate = 0.0001  # 学习率，一般小于0.0001

# train_dir = 'D:/WorkSpace/Dataset/cats_vs_dogs/data/train/'        # 训练集的文件夹路径
# logs_train_dir = 'D:/WorkSpace/work_to_pycharm/cats_vs_dogs/log/'  # 记录训练过程与保存模型的路径
train_dir = 'D:/project/cats_vs_dogs-main/data/train/'                 # 训练集的文件夹路径 "D:\生产实习\cats_vs_dogs-main\data\train"
logs_train_dir = 'D:/project/cats_vs_dogs-main/log/'  # 记录训练过程与保存模型的路径 "D:\生产实习\cats_vs_dogs-main\log"

# 获取要训练的图片和对应的图片标签, 这里返回的train_img是存放猫狗图片路径的列表，train_label是存放对train对应标签的列表(0是猫，1是狗)
train_img, train_label = input_data.get_files(train_dir)

# 读取队列中的数据
train_batch, train_label_batch = input_data.get_batch(train_img, train_label, IMG_W, IMG_H, BATCH_SIZE, CAPACITY)

# 调用model方法得到返回值, 进行变量赋值
train_logits = model.cnn_inference(train_batch, BATCH_SIZE, N_CLASSES)
train_loss = model.losses(train_logits, train_label_batch)
train_op = model.training(train_loss, learning_rate)
train_acc = model.evaluation(train_logits, train_label_batch)

summary_op = tf.summary.merge_all()  # 将所有summary全部保存到磁盘，以便tensorboard显示

accuracy_list = []   # 记录准确率(每50步存一次)
loss_list = []       # 记录损失值(每50步存一次)
step_list = []       # 记录训练步数(每50步存一次)


with tf.Session() as sess:
   sess.run(tf.global_variables_initializer())                        # 变量初始化，如果存在变量则是必不可少的操作

   train_writer = tf.summary.FileWriter(logs_train_dir, sess.graph)   # 用于向logs_train_dir写入summary(训练)的目标文件
   saver = tf.train.Saver()                                           # 用于存储训练好的模型

   # 队列监控（训练的batch数据用到了队列）
   coord = tf.train.Coordinator()   # 创建线程协调器
   threads = tf.train.start_queue_runners(sess=sess, coord=coord)

   try:
       # 执行MAX_STEP步的训练，一步一个batch
       for step in np.arange(MAX_STEP):
           if coord.should_stop():   # 队列中的所有数据已被读出，无数据可读时终止训练
               break

           _op, tra_loss, tra_acc = sess.run([train_op, train_loss, train_acc])   # 在会话中才能读取tensorflow的变量值

           # 每隔50步打印一次当前的loss以及acc，同时记录log，写入writer
           if step % 50 == 0:
               print('Step %d, train loss = %.2f, train accuracy = %.2f%%' % (step, tra_loss, tra_acc * 100.0))
               summary_train = sess.run(summary_op)            # 调用sess.run()，生成的训练数据
               train_writer.add_summary(summary_train, step)   # 将训练过程及训练步数保存

           # 每隔100步画图，记录训练的准确率和损失值的结点
           if step % 100 == 0:
               accuracy_list.append(tra_acc)
               loss_list.append(tra_loss)
               step_list.append(step)

           # 每隔5000步，保存一次训练好的模型（即：训练好的模型的参数保存下来）
           if step % 5000 == 0 or (step + 1) == MAX_STEP:
               # ckpt文件是一个二进制文件，它把变量名映射到对应的tensor值
               checkpoint_path = os.path.join(logs_train_dir, 'model.ckpt')
               saver.save(sess, checkpoint_path, global_step=step)

       plt.figure()                    # 建立可视化图像框
       plt.plot(step_list, accuracy_list, color='b', label='cnn_accuracy')               # 蓝线为准确率
       plt.plot(step_list, loss_list, color='r', label='cnn_loss', linestyle='dashed')   # 红虚线为损失值
       plt.xlabel("Step")              # x轴取名
       plt.ylabel("Accuracy/Loss")     # y轴取名
       plt.legend()                    # 给图加上图例
       plt.show()                      # 显示图片

   except tf.errors.OutOfRangeError:
       print('Done training -- epoch limit reached')
   finally:
       coord.request_stop()   # 停止所有线程

   coord.join(threads)   # 等待所有线程结束
   sess.close()          # 关闭会话

运行结果：
 
   4、实验一下scrapy
运行代码：
"""
   test.py: 用训练好的模型对随机一张图片进行猫狗预测
"""
# import tensorflow as tf
import tensorflow._api.v2.compat.v1 as tf
tf.disable_v2_behavior()
from PIL import Image
import matplotlib.pyplot as plt
import input_data
import model
import numpy as np


def get_one_image(img_list):
   """
       输入：
           img_list：图片路径列表
       返回：
           image：从图片路径列表中随机挑选的一张图片
   """
   n = len(img_list)                  # 获取文件夹下图片的总数
   ind = np.random.randint(0, n)      # 从 0~n 中随机选取下标
   img_dir = img_list[ind]            # 根据下标得到一张随机图片的路径

   image = Image.open(img_dir)        # 打开img_dir路径下的图片
   image = image.resize([208, 208])   # 改变图片的大小，定为宽高都为208像素
   image = np.array(image)            # 转成多维数组，向量的格式
   return image


def evaluate_one_image():
   # 修改成自己测试集的文件夹路径
   test_dir = 'D:/project/cats_vs_dogs-main/data/test/'  # "D:\生产实习\cats_vs_dogs-main\data\test"
   # test_dir = '/home/user/Dataset/cats_vs_dogs/test/'

   test_img = input_data.get_files(test_dir)[0]   # 获取测试集的图片路径列表
   image_array = get_one_image(test_img)          # 从测试集中随机选取一张图片

   # 将这个图设置为默认图，会话设置成默认对话，这样在with语句外面也能使用这个会话执行。
   with tf.Graph().as_default():    # 参考：https://blog.csdn.net/nanhuaibeian/article/details/101862790
       BATCH_SIZE = 1               # 这里我们要输入的是一张图(预测这张随机图)
       N_CLASSES = 2                # 还是二分类(猫或狗)

       image = tf.cast(image_array, tf.float32)                    # 将列表转换成tf能够识别的格式
       image = tf.image.per_image_standardization(image)           # 图片标准化处理
       image = tf.reshape(image, [1, 208, 208, 3])                 # 改变图片的形状
       logit = model.cnn_inference(image, BATCH_SIZE, N_CLASSES)   # 得到神经网络输出层的预测结果
       logit = tf.nn.softmax(logit)                                # 进行归一化处理（使得预测概率之和为1）

       x = tf.placeholder(tf.float32, shape=[208, 208, 3])         # x变量用于占位，输入的数据要满足这里定的shape

       # 修改成自己训练好的模型路径
       logs_train_dir = 'D:/project/cats_vs_dogs-main/log/'

       saver = tf.train.Saver()

       with tf.Session() as sess:
           print("从指定路径中加载模型...")
           ckpt = tf.train.get_checkpoint_state(logs_train_dir)   # 读取路径下的checkpoint
           # 载入模型，不需要提供模型的名字，会通过 checkpoint 文件定位到最新保存的模型
           if ckpt and ckpt.model_checkpoint_path:                # checkpoint存在且其存放的变量不为空
               global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]   # 通过切割获取ckpt变量中的步长
               saver.restore(sess, ckpt.model_checkpoint_path)    # 当前会话中，恢复该路径下模型的所有参数（即调用训练好的模型）
               print('模型加载成功, 训练的步数为： %s' % global_step)
           else:
               print('模型加载失败，checkpoint文件没找到！')

           # 通过saver.restore()恢复了训练模型的参数（即：神经网络中的权重值），这样logit才能得到想要的预测结果
           # 执行sess.run()才能运行，并返回结果数据
           prediction = sess.run(logit, feed_dict={x: image_array})   # 输入随机抽取的那张图片数据，得到预测值
           max_index = np.argmax(prediction)                          # 获取输出结果中最大概率的索引(下标)
           if max_index == 0:
               pre = prediction[:, 0][0] * 100
               print('图片是猫的概率为： {:.2f}%'.format(pre))       # 下标为0，则为猫，并打印是猫的概率
           else:
               pre = prediction[:, 1][0] * 100
               print('图片是狗的概率为： {:.2f}%'.format(pre))       # 下标为1，则为狗，并打印是狗的概率

   plt.imshow(image_array)                                        # 接受图片并处理
   plt.show()                                                     # 显示图片


if __name__ == '__main__':
   # 调用方法，开始测试
   evaluate_one_image()
运行结果：
 
